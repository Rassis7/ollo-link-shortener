# CodeXperience: Escrevendo software de forma eficiente com IA

Abaixo trago um workflow simples para sair do zero a conseguir escrever softwares de forma eficiente 100% guiados √† IA e sem `Vibe Coding`

## Contexto e case

Trabalhamos na empresa OLLO, que √© uma empresa de marketing e est√° desenvolvendo uma solu√ß√£o pr√≥pria de encurtador de link, hoje recebemos do time do produto a seguinte tarefa:

```markdown
# Demanda de Produto ‚Äì Rota de Redirecionamento de Links

Queremos fechar o ciclo do nosso encurtador de links: hoje j√° √© poss√≠vel gerar um link curto, mas ainda n√£o existe uma rota p√∫blica padr√£o para quando algu√©m clica nesse link.

Precisamos de uma rota √∫nica que receba o link encurtado e leve a pessoa automaticamente para a URL original, ou avise claramente quando o link n√£o existir.

## O que esperamos ter ao final:

- Um endere√ßo p√∫blico no formato https://NOSSO_DOMINIO/r/{hash} que possa ser usado em campanhas, e-mails, etc.
- Se o hash existir, deve ser redirecionado para a URL original com o status de redirecionamento correto.
- Se o hash n√£o existir, deve receber uma resposta de ‚Äún√£o encontrado‚Äù (404).
```

Devemos desenvolver guiados √† IA pois temos somente hoje para finalizar a sprint e entregar a demanda.
Esse projeto ainda n√£o tem nada configurado para o uso de IA, ent√£o nosso workflow para inici√°-la e entregar essa tarefa 100% guiada √† IA ser√°:

# Workflow: Do zero a entrega com IA

## Escolher uma ferramenta

Somos livres para escolher a ferramenta que quisermos, iremos usar o Codex pois achamos o `gpt-5.1-codex` bem interessante para essa nossa tarefa e tamb√©m √© mais barato que o Claude Code

## Criando o entrypoint (`AGENTS.MD`)

Isso faz com que o code agent entenda o b√°sico do seu projeto, sempre que for pedir algo para ele ele ir√° nesse arquivo e buscar√° as informa√ß√µes b√°sicas dele

Geralmente os CLI code agents possuem um comando `/init` que gera um arquivo de localiza√ß√£o da ferramenta (`CLAUDE.md, AGENTS.md, GEMINI.md, etc`),

Assim criamos o: [AGENTS.md](../AGENTS.md)

### Mas como manter esse arquivo atualizado caso o time use v√°rias ferramentas?

Assim como fomos livres para escolher o `Codex` como ferramenta de trabalho, h√° outros devs em nossa equipe usando:

- Claude Code
- Gemini CLI
- Github copilot

E cada uma dessas ferramentas possui um arquivo diferente de configura√ß√£o:

- CLAUDE.md
- GEMINI.md
- AGENTS.md

Pensando em sempre manter o mesmo contexto independente da ferramenta podemos criar um link simb√≥lico entre eles

```bash
ln -s AGENTS.md CLAUDE.md
```

```bash
ln -s AGENTS.md GEMINI.md
```

## Expanda o contexto do seu projeto para a IA

Em alguns casos somente o [AGENTS.md](../AGENTS.md) j√° √© suficiente, por√©m podemos expandir ainda mais o contexto para a IA n√£o se perder e conseguir ser assertiva em conseguir obter o contexto da sua aplica√ß√£o.
Para conseguir equilibrar tudo √© interessante criar um banco de mem√≥ria para ela, esse banco de mem√≥ria serve tanto para a IA poder consultar e obter o contexto necess√°rio quanto para humanos manterem o software bem documentado.

N√£o existe uma regra, mas vamos salvar todo o contexto na pasta: `ai-context`

Usei esse prompt ([EXPANDED_CONTEXT.md](./PROMPTS/EXPANDED_CONTEXT.md)) para gerar o contexto

Abaixo est√£o algumas dicas para ser mais efetivo nessa tarefa de expans√£o de contexto

### Separe em macro informa√ß√µes

Quebre a documenta√ß√£o entre os principais aspectos da aplica√ß√£o, como:

```
- arquitetura
- boas pr√°ticas de escrita de c√≥digo
- deploy (CI/CD)
- observabilidade
- testes
```

### Sempre tenha um √≠ndice principal

Com o √≠ndice o modelo ir√° diretamente onde precisa, por exemplo:

Se eu quiser entender sobre uma funcionalidade de envio de email da aplica√ß√£o, posso pedir para a IA:

```bash
> Analise o projeto e explique como fa√ßo para poder enviar um email quando o usu√°rio fechar um pedido.
```

O modelo vai buscar baseado no √≠ndice

```mermaid
graph TB

  AGENTS.md --> AI-CONTEXT["AI-CONTEXT (INDEX)"]
  AI-CONTEXT["AI-CONTEXT (INDEX)"] --> ARQUITETURA
  AI-CONTEXT["AI-CONTEXT (INDEX)"] --> TESTES
  AI-CONTEXT["AI-CONTEXT (INDEX)"] --> INFRA
  AI-CONTEXT["AI-CONTEXT (INDEX)"] --> OBSERVABILIDADE
  ARQUITETURA --> EMAIL["EMAIL MODULE"]
  EMAIL --> CONTROLLER["EMIAL CONTROLLER"]
  EMAIL --> MODEL["EMIAL MODEL"]
  EMAIL --> SERVICE["EMIAL SERVICE"]
  EMAIL --> ANYTHING["EMIAL ???"]
```

Repare que ele n√£o precisou colocar todo o resto no contexto, isso faz o modelo ser mais assertivo, r√°pido e principalmente `ECONOMIZAR TOKENS` üí∞üí∞üí∞üí∞

Olhe a estrutura do [ai-context/index.md](../ai-context/index.md)

### Una seu [AGENTS.md](../AGENTS.md) com o contexto expandido

Linke o [`AGENTS.md`](../AGENTS.md) com o diret√≥rio `ai-context`

```markdown
# Repository Guidelines

> Sempre que precisar de vis√£o geral, arquitetura ou regras espec√≠ficas do projeto, consulte primeiro [`ai-context/index.md`](./ai-context/index.md). Ele centraliza navega√ß√£o r√°pida para todos os resumos preparados para agentes e humanos.

## Project Structure & Module Organization

[...]
```

## O que precisamos de saber antes de nos aventurar a "codar" guiados √† IA?

J√° sabemos o que o time de produto quer e temos a IA configurada no nosso projeto, agora devemos desenvolver a tarefa de fato, mas precisamos entender alguns conceitos antes de pedir para o modelo desenvolver qualquer coisa.

#### Mem√≥ria

Mas antes de tudo, precisamos entender alguns pontos: os modelos de LLM possuem mem√≥ria de curto e longo prazo.

- Curto prazo: √© o contexto imediato da conversa (janela de tokens atual), tudo que o modelo ‚Äúv√™‚Äù e usa para responder naquele turno.
- Longo prazo: √© qualquer armazenamento externo ou ajuste permanente (banco de dados, RAG, fine-tuning, ou at√© um arquivo) usado para reintroduzir informa√ß√µes relevantes em conversas futuras.

Sabendo disso, entendemos que: se pedirmos para criar a tarefa e somente isso o modelo ir√° colocar isso na janela de contexto (curto prazo), mas se ele encher ela pode se perder durante o processo, gerando c√≥digo que n√£o queremos ou inventando regras de neg√≥cio.

**Precisamos garantir que ele use mem√≥ria de longo prazo para n√£o se perder durante tarefas futuras**

#### Plan mode VS Act mode

O modelo que estamos usando √© o `gpt-5.1-codex`; ele √© um modelo que possui reasoning por padr√£o, isso significa que o modelo consegue "raciocinar" antes de tomar alguma decis√£o.
Algumas ferramentas, como o `Claude Code`, possuem uma op√ß√£o para o modelo pensar antes de executar; o `Codex` n√£o possui isso por padr√£o.

**√â extremamente eficiente e recomendado pedir para o modelo planejar a tarefa antes de executar**

#### Uso de TDD

TDD √© visto como uma boa pr√°tica na programa√ß√£o; desenvolver guiado √† IA com TDD tem vantagens, pois antes do modelo escrever qualquer linha de implementa√ß√£o, ele pensa nos casos de teste e os escreve, somente ap√≥s isso ele implementa.

**Usando TDD os modelos nos d√£o uma certa garantia que eles pensaram antes em como deve ser implementado antes de implementar**

#### Nunca aceitar o que a IA est√° sugerindo sem validar

Somos engenheiros e n√£o devemos aceitar cegamente o que uma ferramenta est√° sugerindo, mesmo com todo o contexto que iremos prover para o modelo ele ir√° errar em algo.

**Sempre veja e garanta que o c√≥digo escrito pela IA √© um c√≥digo bom, se n√£o for, intervenha e o fa√ßa do melhor jeito poss√≠vel**

### Regras para executar uma tarefa guiada √† IA

Como entendemos alguns conceitos e dicas, agora temos as regras:

- Precisamos garantir que ele use mem√≥ria de longo prazo para n√£o se perder durante tarefas futuras.
- √â extremamente eficiente e recomendado pedir para o modelo planejar a tarefa antes de executar.
- Usando TDD os modelos nos d√£o uma certa garantia que eles pensaram antes em como deve ser implementado antes de implementar.
- Sempre veja e garanta que o c√≥digo escrito pela IA √© um c√≥digo bom, se n√£o for, intervenha e fa√ßa do melhor jeito poss√≠vel.

Agora que temos as regras de ouro, vamos criar um modelo de plano de execu√ß√£o de tarefas.
O que isso significa: sempre que formos pedir para o modelo executar alguma tarefa ele dever√° planejar antes usando o modelo que iremos desenvolver, assim garantimos que ela tenha uma **mem√≥ria de longo prazo**, **planeje antes de executar**, **siga o TDD**, e ganhamos com isso checkpoints durante o desenvolvimento, ou seja, se o modelo **escreve um c√≥digo ruim** podemos voltar √† tarefa que desejamos e alterar o que for necess√°rio.

Aqui temos o arquivo [PROMPTS/TEMPLATES/TEMPLATE_EXECUTION_PLAN.md](PROMPTS/TEMPLATES/TEMPLATE_EXECUTION_PLAN.md) que ser√° nosso modelo.

## Vamos finalmente √† tarefa...

J√° sabemos o b√°sico, agora vamos entregar a tarefa.
Nosso time de produto foi claro na tarefa **"criar uma rota para o usu√°rio ser redirecionado"**

Ok, ent√£o vamos criar nosso prompt adicionando a ele todas as regras de neg√≥cio e indicando que ele deve _criar um plano de execu√ß√£o_... ele est√° aqui [PROMPTS/PROMPT_TASK.md](PROMPTS/PROMPT_TASK.md)

O executar o prompt foi criado o plano de execu√ß√£o dessa tarefa [CODEXPERIENCE/PLANS/REDIRECTOR_PLAN.md](CODEXPERIENCE/PLANS/REDIRECTOR_PLAN.md)

> Observe que esse plano j√° est√° atualizado, pois a tarefa j√° foi executada, como ele √© a mem√≥ria do modelo ele precisa atualizar ele ao fim de cada tarefa dando contexto do que foi feito

### Code review

Claro, n√£o podemos esquecer do code review, uma boa pr√°tica √© pedir para o pr√≥prio modelo fazer o code review.
As ferramentas como Codex e Claude Code j√° possuem isso nativamente, eles fazem em um outro contexto n√£o tendo influ√™ncia em tarefas passadas.

No Codex basta rodar o comando `/review` e ele o far√°...

### Pull request

Perfeito, olhe o [PR](https://github.com/Rassis7/ollo-link-shortener/pull/32), o c√≥digo est√° l√° üëÄ

# Entrega efetuada!!

Pronto, sa√≠mos do zero at√© um workflow extremamente eficiente para o desenvolvimento guiado √† IA.

Obrigado por me acompanhar nessa tarefa e at√© mais! ‚ù§Ô∏è
